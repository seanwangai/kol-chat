from utils.quota import check_quota, use_quota, get_quota_display  # ä½¿ç”¨æ–°çš„å‡½æ•°å
from openai import OpenAI
from openai import APIError, APIConnectionError, RateLimitError, APITimeoutError
from openai import AsyncOpenAI  # æ”¹ç”¨å¼‚æ­¥å®¢æˆ·ç«¯
import logging
import time
import tiktoken
import asyncio
from concurrent.futures import ThreadPoolExecutor
import streamlit as st
import backoff  # æ·»åŠ åˆ°å¯¼å…¥åˆ—è¡¨
from datetime import datetime, timedelta
import sys
import os
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type
)
import random

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# åœ¨æ–‡ä»¶å¼€å¤´æ·»åŠ æ›´è¯¦ç»†çš„æ—¥å¿—æ ¼å¼è®¾ç½®
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)

# X-AI API é…ç½®
client = AsyncOpenAI(  # æ”¹ç”¨å¼‚æ­¥å®¢æˆ·ç«¯
    api_key=st.secrets.get("XAI_API_KEY", ""),
    base_url=st.secrets.get("XAI_API_BASE", "https://api.x.ai/v1")
)

# åˆ›å»ºçº¿ç¨‹æ± 
executor = ThreadPoolExecutor(max_workers=10)

# è·å– token è®¡æ•°å™¨
encoding = tiktoken.get_encoding("cl100k_base")  # GPT-4 ä½¿ç”¨çš„ç¼–ç å™¨

MAX_TOKENS = 131072  # Grok æœ€å¤§ token é™åˆ¶
SYSTEM_PROMPT_TEMPLATE = """ä½ æ˜¯è‘—åæ–‡æ¡ˆå°ˆå®¶

{knowledge}

"""


def truncate_text(text, max_tokens):
    """æˆªæ–­æ–‡æœ¬ä»¥ç¡®ä¿ä¸è¶…è¿‡æœ€å¤§ token é™åˆ¶"""
    tokens = encoding.encode(text)
    if len(tokens) <= max_tokens:
        return text

    # è®¡ç®—éœ€è¦ä¿ç•™çš„ä¸­é—´éƒ¨åˆ†çš„ token æ•°é‡
    middle_tokens = max_tokens

    # è®¡ç®—å¼€å§‹å’Œç»“æŸçš„ä½ç½®
    total_tokens = len(tokens)
    remove_tokens = total_tokens - middle_tokens

    # å‰é¢ä¿ç•™æ›´å¤šå†…å®¹ï¼ˆ70%ï¼‰ï¼Œåé¢å°‘ä¸€äº›ï¼ˆ30%ï¼‰
    remove_front = int(remove_tokens * 0.3)
    remove_back = remove_tokens - remove_front

    # ä¿ç•™ä¸­é—´éƒ¨åˆ†çš„ tokens
    start_idx = remove_front
    end_idx = total_tokens - remove_back

    # è®°å½•æˆªæ–­ä¿¡æ¯
    logger.info(f"æ–‡æœ¬è¢«æˆªæ–­ï¼šæ€»tokens={total_tokens}, "
                f"ä¿ç•™tokens={middle_tokens}, "
                f"å‰é¢åˆ é™¤={remove_front}, "
                f"åé¢åˆ é™¤={remove_back}")

    # è¿”å›æˆªæ–­åçš„æ–‡æœ¬
    return (
        f"...[å‰é¢å·²çœç•¥ {remove_front} tokens]...\n\n" +
        encoding.decode(tokens[start_idx:end_idx]) +
        f"\n\n...[åé¢å·²çœç•¥ {remove_back} tokens]..."
    )


logger = logging.getLogger(__name__)


# æ·»åŠ è¯·æ±‚é™åˆ¶ç®¡ç†
class RateLimiter:
    def __init__(self, requests_per_second=1):
        self.requests_per_second = requests_per_second
        self.last_request_time = None
        self._lock = None

    @property
    def lock(self):
        if self._lock is None:
            self._lock = asyncio.Lock()
        return self._lock

    async def acquire(self):
        async with self.lock:
            now = datetime.now()
            if self.last_request_time is not None:
                time_since_last = (
                    now - self.last_request_time).total_seconds()
                if time_since_last < 1/self.requests_per_second:
                    wait_time = 1/self.requests_per_second - time_since_last
                    await asyncio.sleep(wait_time)
            self.last_request_time = datetime.now()


# åˆ›å»ºå…¨å±€é™é€Ÿå™¨å®ä¾‹
rate_limiter = RateLimiter(requests_per_second=1)


class Expert:
    def __init__(self, name):
        self.name = name
        self.background = self._load_background()  # åˆå§‹åŒ–æ™‚å°±è®€å–èƒŒæ™¯è³‡æ–™
        self.chat_history = []

    def _load_background(self):
        try:
            with open(f"data/{self.name}/data.txt", "r", encoding="utf-8") as f:
                background = f.read().strip()
                # è¨˜éŒ„è¼‰å…¥çš„èƒŒæ™¯è³‡æ–™
                logger.info({
                    "action": "load_expert_background",
                    "expert": self.name,
                    "background_length": len(background),
                    "background_preview": background[:200] + "..." if len(background) > 200 else background
                })
                return background
        except Exception as e:
            logger.error(f"Error loading background for {self.name}: {e}")
            return ""

    def get_system_prompt(self):
        return f"""ä½ ç¾åœ¨æ‰®æ¼”çš„æ˜¯{self.name}ã€‚ä»¥ä¸‹æ˜¯ä½ çš„èƒŒæ™¯è³‡æ–™ï¼š

{self.background}

è«‹ä¾æ“šä»¥ä¸ŠèƒŒæ™¯ä¾†å›ç­”å•é¡Œã€‚è«‹ç”¨çœŸèª ã€å°ˆæ¥­çš„æ…‹åº¦ä¾†å›ç­”ã€‚
"""


class ExpertAgent:
    def __init__(self, name, knowledge_base, avatar=None):
        self.name = name
        self.original_knowledge = knowledge_base
        self.avatar = avatar or "ğŸ¤–"
        self.chat_history = []
        self.max_history = 5
        self.history_tokens = 0

        # å‰µå»º Expert å¯¦ä¾‹ï¿½ï¿½ç®¡ç†èƒŒæ™¯è³‡æ–™
        self.expert = Expert(name)

        # è¨ˆç®—åŸºæœ¬ token
        self.base_tokens = len(encoding.encode(
            self.expert.get_system_prompt()))
        self.tokens_per_turn = 2000

    def count_tokens(self, text):
        """è®¡ç®—æ–‡æœ¬çš„ token æ•°é‡"""
        return len(encoding.encode(text))

    def adjust_knowledge_base(self):
        """æ ¹æ®å¯¹è¯å†å²åŠ¨æ€è°ƒæ•´çŸ¥è¯†åº“å¤§å°"""
        # è®¡ç®—å¯ç”¨äºçŸ¥è¯†åº“çš„ tokens
        available_tokens = (MAX_TOKENS - self.base_tokens -
                            self.history_tokens - self.tokens_per_turn)

        # ç¡®ä¿è‡³å°‘ä¿ç•™å®šæ¯”ä¾‹çš„çŸ¥è¯†åº“å†…å®¹
        min_knowledge_tokens = min(
            80000, available_tokens)  # æé«˜æœ€å°ä¿ç•™é‡åˆ°80k tokens
        max_knowledge_tokens = max(min_knowledge_tokens, available_tokens)

        # æˆªæ–­çŸ¥è¯†åº“å†…å®¹
        self.knowledge_base = truncate_text(
            self.original_knowledge, max_knowledge_tokens)

        # è®°å½•è°ƒæ•´ä¿¡æ¯
        logger.info(f"çŸ¥è¯†åº“è°ƒæ•´ï¼šå†å²tokens={self.history_tokens}, "
                    f"å¯ç”¨tokens={available_tokens}, "
                    f"åˆ†é…ç»™çŸ¥è¯†åº“tokens={max_knowledge_tokens}")

    def get_system_prompt(self):
        """è·å–å½“å‰çš„ç³»ç»Ÿæç¤ºè¯"""
        # ä½¿ï¿½ï¿½ï¿½ Expert é¡çš„ç³»çµ±æç¤º
        return self.expert.get_system_prompt()

    def update_chat_history(self, question, answer):
        """æ–°å¯¹è¯å†å²"""
        # è®¡ç®—æ–°å¯¹è¯çš„ tokens
        new_qa_tokens = self.count_tokens(f"Q: {question}\nA: {answer}")

        # å¦‚æœéœ€è¦ç§»é™¤æ—§å¯¹è¯
        while (self.history_tokens + new_qa_tokens > MAX_TOKENS * 0.3 and  # å†å²æœ€å¤šå ç”¨30%
               self.chat_history):
            # ç§»é™¤æœ€æ—©çš„å¯¹è¯å¹¶å‡å°‘ token è®¡æ•°
            old_q, old_a = self.chat_history.pop(0)
            removed_tokens = self.count_tokens(f"Q: {old_q}\nA: {old_a}")
            self.history_tokens -= removed_tokens
            logger.info(f"ç§»é™¤æ—§å¯¹è¯ï¼Œé‡Šæ”¾ {removed_tokens} tokens")

        # æ·»åŠ æ–°å¯¹è¯
        self.chat_history.append((question, answer))
        self.history_tokens += new_qa_tokens

        logger.info(f"æ·»åŠ æ–°å¯¹è¯ï¼Œä½¿ç”¨ {new_qa_tokens} tokensï¼Œ"
                    f"å½“å‰å†å²æ€»è®¡ {self.history_tokens} tokens")

        self.adjust_knowledge_base()  # é‡æ–°è°ƒæ•´çŸ¥è¯†åº“å¤§å°

    # ä¿®æ”¹è£…é¥°å™¨
    @retry(
        retry=retry_if_exception_type(
            (APIConnectionError, APITimeoutError, RateLimitError)),
        wait=wait_exponential(multiplier=1, min=1, max=10),
        stop=stop_after_attempt(3)
    )
    async def get_response(self, prompt):
        """è·å–ä¸“å®¶å›åº”"""
        try:
            logger.info(f"å¼€å§‹å¤„ç†å®¶ {self.name} çš„å›åº”")
            current_model = getattr(
                st.session_state, 'current_model', 'grok-beta')

            # æ¯æ¬¡å°è©±éƒ½æœƒå¸¶å…¥å®Œæ•´çš„ç³»çµ±æç¤º
            messages = [
                {
                    "role": "system",
                    "content": self.expert.get_system_prompt()  # åŒ…å«å°ˆå®¶èƒŒæ™¯
                }
            ]

            # ç„¶å¾ŒåŠ å…¥æ­·å²å°è©±
            for old_q, old_a in self.chat_history:
                messages.append({"role": "user", "content": old_q})
                messages.append({"role": "assistant", "content": old_a})

            # æœ€å¾ŒåŠ å…¥ç•¶å‰å•é¡Œ
            messages.append({"role": "user", "content": prompt})

            # è¨˜éŒ„è«‹æ±‚å…§å®¹
            logger.info({
                "action": "send_to_ai_api",
                "expert": self.name,
                "model": current_model,
                "request_data": {
                    "messages": messages,
                    "temperature": 0.7,
                    "max_tokens": MAX_TOKENS,
                    "system_prompt": messages[0]["content"],  # å®Œæ•´è¨˜éŒ„ç³»çµ±æç¤º
                    "history_length": len(self.chat_history),
                    "history_tokens": self.history_tokens
                },
                "timestamp": datetime.now().isoformat()
            })

            try:
                await rate_limiter.acquire()
                response = await client.chat.completions.create(
                    model="grok-beta",
                    messages=messages,
                    temperature=0.7
                )
                answer = response.choices[0].message.content
            except Exception as e:
                logger.error(f"Grok API è°ƒç”¨å¤±è´¥: {str(e)}")
                raise

            # è¨˜éŒ„å›æ‡‰å…§å®¹
            logger.info({
                "action": "receive_from_ai_api",
                "expert": self.name,
                "model": current_model,
                "response_data": {
                    "content_length": len(answer),
                    "content_preview": answer[:200] + "..." if len(answer) > 200 else answer
                },
                "timestamp": datetime.now().isoformat()
            })

            self.update_chat_history(prompt, answer)
            return answer

        except Exception as e:
            logger.error({
                "action": "api_call_error",
                "expert": self.name,
                "model": current_model,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            })
            raise


async def get_responses_async(experts, prompt):
    start_time = time.time()
    logger.info(f"å¼€å§‹å¹¶å‘å¤„ç†æ‰€æœ‰ä¸“å®¶å›åº”ï¼Œæ—¶é—´: {start_time}")

    # ç²å–ç•¶å‰äº‹ä»¶å¾ªç’°
    try:
        loop = asyncio.get_event_loop()
    except RuntimeError:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)

    async def get_expert_response(expert):
        try:
            response = await expert.get_response(prompt)
            return expert, response, time.time()
        except Exception as e:
            logger.error(f"ä¸“å®¶ {expert.name} å¤„ç†å¤±è´¥: {str(e)}")
            return expert, f"æŠ±æ­‰ï¼Œç”Ÿæˆå›åº”æ—¶å‡ºç°é”™è¯¯: {str(e)}", time.time()

    # å‰µå»ºæ‰€æœ‰ä»»å‹™
    tasks = []
    for expert in experts:
        try:
            # ç¢ºä¿ä½¿ç”¨åŒä¸€å€‹äº‹ä»¶å¾ªç’°
            task = loop.create_task(get_expert_response(expert))
            tasks.append(task)
        except Exception as e:
            logger.error(f"åˆ›å»ºä»»åŠ¡å¤±è´¥: {str(e)}")
            continue

    if not tasks:
        logger.error("æ²¡æœ‰æˆåŠŸåˆ›å»ºä»»ä½•ä»»åŠ¡")
        return

    try:
        # ä½¿ç”¨ gather è€Œä¸æ˜¯ as_completed
        responses = await asyncio.gather(*tasks, return_exceptions=True)

        # è™•ç†æ¯å€‹å›æ‡‰
        for expert, response, finish_time in responses:
            if isinstance(response, Exception):
                logger.error(f"ä¸“å®¶ {expert.name} å¤„ç†å¤±è´¥: {str(response)}")
                yield expert, f"æŠ±æ­‰ï¼Œç”Ÿæˆå›åº”æ—¶å‡ºç°é”™è¯¯: {str(response)}"
            else:
                logger.info(
                    f"ä¸“å®¶ {expert.name} å“åº”å®Œæˆï¼Œè€—æ—¶: {finish_time - start_time:.2f}ç§’")
                yield expert, response

        # ç”Ÿæˆç¸½çµ
        try:
            # éæ¿¾å‡ºæˆåŠŸçš„å›æ‡‰
            valid_responses = [
                (e, r) for e, r, _ in responses if not isinstance(r, Exception)]
            if valid_responses:
                experts_for_summary, responses_for_summary = zip(
                    *valid_responses)
                summary = await generate_summary(prompt, responses_for_summary, experts_for_summary)
                yield st.session_state.titans, summary
            else:
                logger.error("æ²¡æœ‰æˆåŠŸçš„å›åº”å¯ä»¥ç”Ÿæˆæ€»ç»“")
                yield st.session_state.titans, "æŠ±æ­‰ï¼Œç”±äºæ‰€æœ‰ä¸“å®¶å›åº”éƒ½å¤±è´¥ï¼Œæ— æ³•ç”Ÿæˆæ€»ç»“ã€‚"
        except Exception as e:
            logger.error(f"ç”Ÿæˆæ€»ç»“æ—¶å‡ºé”™: {str(e)}")
            yield st.session_state.titans, "æŠ±æ­‰ï¼Œç”Ÿæˆæ€»ç»“æ—¶å‡ºç°é”™è¯¯ã€‚"

    except Exception as e:
        logger.error(f"å¤„ç†å“åº”è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
        raise


async def generate_summary(prompt, responses, experts):
    """ç”Ÿæˆæ€»ç»“"""
    logger.info("å¼€å§‹ç”Ÿæˆæ€»ç»“...")

    # å‹•æ…‹æ§‹å»ºå°ˆå®¶å›æ‡‰åˆ—è¡¨
    expert_responses = []
    for expert, response in zip(experts, responses):
        expert_responses.append(f"{expert.name}ï¼š{response}")
        logger.info(f"æ•´åˆ {expert.name} çš„å›æ‡‰åˆ°ç¸½çµä¸­")

    # å‰µå»ºä¸€å€‹å°ˆé–€çš„æ•´åˆå°ˆå®¶
    summary_expert = Expert("æ–‡æ¡ˆæ•´åˆå°ˆå®¶")

    # æ§‹å»ºæ¶ˆæ¯åˆ—è¡¨ï¼Œç¢ºä¿ç³»çµ±æç¤ºåœ¨æœ€å‰é¢
    messages = [
        {
            "role": "system",
            "content": summary_expert.get_system_prompt()
        },
        {
            "role": "user",
            "content": f"""çµåˆå„æ–‡ç« çš„å„ªé»ï¼Œæ”¹å¯«å‡ºä¸€ç¯‡æœ€çµ‚æ–‡æ¡ˆï¼š

{chr(10).join(expert_responses)}
"""
        }
    ]

    logger.info({
        "action": "generate_summary",
        "system_prompt": messages[0]["content"],
        "user_prompt": messages[1]["content"][:200] + "..."
    })

    try:
        # ä½¿ç”¨å¼‚æ­¥ API è°ƒç”¨
        summary_response = await client.chat.completions.create(
            model="grok-beta",
            messages=messages,
            temperature=0.7
        )
        summary = summary_response.choices[0].message.content
        return summary
    except Exception as e:
        error_msg = "ç”Ÿæˆæ€»ç»“æ—¶å‡ºé”™"
        logger.error(error_msg)
        logger.exception(e)
        return "æŠ±æ­‰ï¼Œæ— æ³•ç”Ÿæˆæ€»ç»“ã€‚"

__all__ = ['ExpertAgent', 'get_responses_async', 'generate_summary']
